{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "private_outputs": true,
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAZvnGs_0YIn"
      },
      "source": [
        "### Connecting Google Colab with your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4qX0VFDtPew"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tq2H1dTxF56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/nlp/textpre/Text_Preprocessing\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "UeHw-Rwu84OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iySby66xrKar"
      },
      "source": [
        "# <center>Text Preprocessing</center>\n",
        "\n",
        "Text data falls into the category of unstructured data and requires some preparation before it can be used for modeling. Text preparation is different from structured data pre-processing. In this activitiy we will be using spaCy to do the same.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnRje45Dsru7"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='./img/spacy_img.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52OalupXsTxg"
      },
      "source": [
        "\n",
        "## Why spaCy?\n",
        "\n",
        "* Is a free and open-source library developed by Explosion AI.\n",
        "* Works well for simple to complex language understanding tasks and is designed specifically for production use.\n",
        "* Helps build applications that process and “understand” large volumes of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnuo9jVEsT1K"
      },
      "source": [
        "### Features of spaCy\n",
        "\n",
        "spaCy provides [trained models](https://spacy.io/models/en) for different languages and has a model for multi-language as well.\n",
        "\n",
        "Before jumping in, let's have a look at various features provided by popular NLP related libraries and their performance in compairision to SpaCy.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR3RbHrzzRRl"
      },
      "source": [
        "Image(filename='./img/Spacy_Features.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW95zuqRsT4U"
      },
      "source": [
        "### Feature Comparision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIvVd9ETzvel"
      },
      "source": [
        "Image(filename='./img/feature-comparision.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alwn6BsvsULL"
      },
      "source": [
        "### Speed Comparision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SELIGIRfz4JS"
      },
      "source": [
        "Image(filename='./img/speed-comparision.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo4kIr_drKa2"
      },
      "source": [
        "### SpaCy Installation\n",
        "\n",
        "To get started with spaCy, install the package using pip in Terminal (for Mac) or CommandLine (for Windows).\n",
        "\n",
        "__Note__: No separate installation of spaCy is required to execute in colab.\n",
        "\n",
        "The language pre-trained model packages can be downloaded using the \"spacy download\" command. We will download `en_core_web_sm` package\n",
        "\n",
        "- en = English\n",
        "- core = Core (Vocab, Syntax, Entities, Vectors)\n",
        "- web = Web Text\n",
        "- sm/md/lg = Small/Medium/Large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u93r71-inXB8"
      },
      "source": [
        "``` Python\n",
        "!pip install -U spacy\n",
        "\n",
        "(or)\n",
        "\n",
        "!conda install -c conda-forge spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZwddm2KrKa7"
      },
      "source": [
        "import spacy\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsqFC6F1rKa7"
      },
      "source": [
        "# Load the pre trained language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Incase if the above command fails please uncomment the below lines and execute the code.\n",
        "# import en_core_web_sm\n",
        "# nlp = en_core_web_sm.load()\n",
        "\n",
        "# Create SpaCy Object\n",
        "doc = nlp(\"Hello World\")\n",
        "\n",
        "# Print the document text\n",
        "print(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# doc1 = nlp(string)\n",
        "# doc1"
      ],
      "metadata": {
        "id": "lmIEpgZvKNiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAxbTy3ErKa9"
      },
      "source": [
        "#### We will be using a sample text to demonstrate various text pre-processing steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_by_zZBarKa-"
      },
      "source": [
        "string = '''At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
        "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
        "My companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
        "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
        "The train was @09:30 AM and we have to reach the station by 08:30 AM. At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
        "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdPa6tzyrKa-"
      },
      "source": [
        "string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOK_bRdWrKa_"
      },
      "source": [
        "type(string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw6D2txtrKa_"
      },
      "source": [
        "len(string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y2YxExxrKbA"
      },
      "source": [
        "string.count(\"Waterloo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string[5]"
      ],
      "metadata": {
        "id": "KmclNm7A3NQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJK0DxD8SWPa"
      },
      "source": [
        "string[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRmoJGQirKbA"
      },
      "source": [
        "#### Usually we will reading the data from files. So let's do the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSVVZjfmrKbA"
      },
      "source": [
        "# Change the working directory using \".chdir()\" method\n",
        "PATH = os.getcwd()\n",
        "\n",
        "DATA_PATH = os.path.join(PATH, \"data\")\n",
        "\n",
        "os.chdir(DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH"
      ],
      "metadata": {
        "id": "msIPjxgQJ5ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWw5SR4jrKbB"
      },
      "source": [
        "# We can verify the files that are present in the path\n",
        "print(os.listdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDdEK8VFrKbB"
      },
      "source": [
        "# Reading from a text file\n",
        "with open('sample_text.txt', 'r') as f:\n",
        "    string = f.read()\n",
        "\n",
        "# 'r' in the code stands for read operation. One would use 'w' to write to a file and 'a' to append to an existing file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ6HB75FrKbB"
      },
      "source": [
        "string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4yxTJQYrKbC"
      },
      "source": [
        "### Now that our sample text is ready, let us perform the following steps:\n",
        "\n",
        "1. Sentence Tokenizing\n",
        "2. Word Tokenizing\n",
        "3. Stop Word Removal\n",
        "4. Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB3N7o6grKbC"
      },
      "source": [
        "# Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB-op9qSrKbC"
      },
      "source": [
        "#### Sentence tokenize - splits entire text to sentences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# string=\"a boy hello 1#\"\n",
        "string=\"sentence tokenization is Done. It is a part of preprocessing\""
      ],
      "metadata": {
        "id": "Jd5XEEQe5bSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzggWDf6rKbD"
      },
      "source": [
        "doc = nlp(string)                         # doc is as spacy object\n",
        "\n",
        "sent_tokens = [w.text for w in doc.sents] # list comprehensions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(doc.sents)"
      ],
      "metadata": {
        "id": "mT7RKH_M5JaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl__Uhf2rKbD"
      },
      "source": [
        "len(sent_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d75AOuC6rKbD"
      },
      "source": [
        "for sent in sent_tokens:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nrt8Nr2rKbE"
      },
      "source": [
        "#### word tokenize - splits strings to words and separates punctuations also"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFjqLMoGrKbE"
      },
      "source": [
        "tokens = []\n",
        "for token in doc:\n",
        "  # print(token.text)\n",
        "    tokens.append(token.text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tok(stri):\n",
        "  doc = nlp(stri)\n",
        "  tokens = []\n",
        "  for token in doc:\n",
        "      tokens.append(token.text)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "4c6DfTam8xtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5YaBZ49euDp"
      },
      "source": [
        "# Just for the explanation\n",
        "\n",
        "for token in doc:\n",
        "  print(token, \":\", type(token))\n",
        "  print(token.text, \":\", type(token.text))\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P9aCVu-rKbE"
      },
      "source": [
        "print(tokens[0:11])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yklwPhfprKbE"
      },
      "source": [
        "## Regular-Expression Tokenizers\n",
        "\n",
        "**What is Regular Expression?**\n",
        "\n",
        "A RegEx or Regular Expression in a programming language is a special text string used for describing a search pattern. It is extremely useful for extracting information from text such as code, files, log, spreadsheets or even documents.\n",
        "\n",
        "While using the regular expression the first thing is to recognize is that everything is essentially a character, and we are writing patterns to match a specific sequence of characters also referred as string. Ascii or latin letters are those that are on your keyboards and Unicode is used to match the foreign text. It includes digits and punctuation and all special characters like $#@!%, etc.\n",
        "\n",
        "**Lets try Regular Expressions using \"re\"**\n",
        "\n",
        "__re__ module included with Python is primarily used for string searching and manipulation. It is quite useful for text extraction and pre-processing. The most common use for __re__ is to search for patterns in text.\n",
        "\n",
        "A __re__ splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of alphabetic sequences, money expressions, and any other non-whitespace sequences:('\\w+|$[\\d.]+|\\S+') For more information or different variations - https://docs.python.org/3/library/re.html\n",
        "\n",
        "https://spacy.io/usage/rule-based-matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3mHy2ELrKbF"
      },
      "source": [
        "**Regular Expressions**\n",
        "1. `\\d` - Matches any decimal digit; this is equivalent to the class [0-9].\n",
        "2. `\\D` - Matches any non-digit character; this is equivalent to the class [^0-9].\n",
        "3. `\\s` - Matches any whitespace character;\n",
        "4. `\\S` - Matches any non-whitespace character;\n",
        "5. `\\w` - Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\n",
        "6. `\\W` - Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
        "\n",
        "The special characters are:\n",
        "\n",
        "7. `.` - (Dot) In the default mode, this matches any character except a newline. If the DOTALL flag has been specified, this matches any character including a newline.\n",
        "8. `^` - (Caret) Matches the start of the string, and in MULTILINE mode also matches immediately after each newline.\n",
        "9. `$` - Matches the end of the string or just before the newline at the end of the string, and in MULTILINE mode also matches before a newline.\n",
        "10. `*` - Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible. ab* will match ‘a’, ‘ab’, or ‘a’ followed by any number of ‘b’s.\n",
        "11. `+` - Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’.\n",
        "12. `?` - Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. ab? will match either ‘a’ or ‘ab’.\n",
        "13. `\\` - Either escapes special characters (permitting you to match characters like `*`, `?`, and so forth), or signals a special sequence; special sequences are discussed below.\n",
        "14. `[]` - Used to indicate a set of characters. In a set: Example: Characters can be listed individually, e.g. [amk] will match 'a', 'm', or 'k'.\n",
        "15. `|` - A|B, where A and B can be arbitrary REs, creates a regular expression that will match either A or B. An arbitrary number of REs can be separated by the `|` in this way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDBcvvPHrKbG"
      },
      "source": [
        "### E.g. to understand Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4pUg68JuKvu"
      },
      "source": [
        "Matches one or more occurence of alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "brcuw5zMrKbF"
      },
      "source": [
        "print([w for w in tokens if re.search('\\w+', w)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([w for w in tokens if re.search('\\w', w)])"
      ],
      "metadata": {
        "id": "TjkjqlTp4hk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mDgL3vTubsL"
      },
      "source": [
        "Matches a string that starts with capital letter followed by zero or more occurence of alphanumeric character; this is equivalent to the class [a-zA-Z0-9_]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHypiNB0rKbG"
      },
      "source": [
        "print([w for w in tokens if re.search('[A-Z]\\w*', w)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qS7zjPTa8k9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=tok(\"deekshitha95\")"
      ],
      "metadata": {
        "id": "dlr6wOmw8lSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ3sCTqNuu7u"
      },
      "source": [
        "Matches a string that contains \"ee\" in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPcoZXP9rKbH"
      },
      "source": [
        "[w for w in tokens if re.search('\\w*ee\\w*', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvbljrnYu74Y"
      },
      "source": [
        "Matches a string that contains digits in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pv50CMxErKbI"
      },
      "source": [
        "[w for w in tokens if re.search('\\d+', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tCj9IcarKbJ"
      },
      "source": [
        "text = nlp('''International School of Engineering (INSOFE) is an Applied Engineering school with area of focus in Data Science. It is located in Hyderabad, Bengaluru and Mumbai. It opened in 2011.\n",
        "The program is delivered through classroom only sessions and is suitable for students and working professionals. Dr. Dakshinamurthy V Kolluru, Dr. Sridhar Pappu and A S L Ganapathi Kumar started the institution in Hyderabad in mid-2011 and expanded to Bengaluru in early-2016. Initially the school functioned under mentorship of Dr. Dakshinamurthy, Dr. Sridhar and Dr. Sreerama Murthy. They are now supported by a team of additional mentors and in-house data scientists.\n",
        "In 2012, INSOFE also started Corporate training services. It extended operations to Bengaluru in 2016. CIO.com listed INSOFE 3rd in their list of \"16 Big Data Certifications That Will Pay Off\" consecutively from 2013-2016. Silicon India Magazine listed INSOFE in their list of \"Top 5 Big Data Training Institutes 2016\". Analytics India Magazine, listed INSOFE in \"Top 9 Analytics Training Institutes in India in 2016\". KDnuggets mentioned INSOFE in their list of Certificates in Analytics, Data Mining, and Data Science in 2014.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIh22uwjvUxA"
      },
      "source": [
        "Tokenize the text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQCGbfTlrKbK"
      },
      "source": [
        "tokens1 = [token.text for token in text]\n",
        "print(tokens1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZxkYLxAwWzE"
      },
      "source": [
        "Match pattern starting with I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4wbieShrKbL"
      },
      "source": [
        "print([w for w in tokens1 if re.search('^I', w)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op3WNDwawihn"
      },
      "source": [
        "Match all tokens that ends with either `ing` or `uru`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiHRY0QtrKbM"
      },
      "source": [
        "[w for w in tokens1 if re.search('ing$|uru$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[w for w in tokens1 if re.search('!ing$|uru$', w)]"
      ],
      "metadata": {
        "id": "7mqG3_FA7Nwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W2E2CZ8wzG8"
      },
      "source": [
        "Match all tokens that starts with H, B or M char"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0b7MoxLrKbM"
      },
      "source": [
        "[w for w in tokens1 if re.search('^[H|B|M]', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA3DzlrZ0ggL"
      },
      "source": [
        "Search for words - Hyderabad, Bengaluru and Mumbai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k2qjEzYrKbN"
      },
      "source": [
        "[w for w in tokens1 if re.search('^Hyd|Ben|Mum', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[w for w in tokens1 if re.search('^Hyd|Ben|Mum', w)]"
      ],
      "metadata": {
        "id": "Z3-GkfrN_pNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOGlcufh0wsW"
      },
      "source": [
        "Search for the word 'Data' or 'Analytics or Science'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe8SLZdMrKbN"
      },
      "source": [
        "[w for w in tokens1 if re.search('Data|Ana|Sci', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4scMPlG00-M"
      },
      "source": [
        "Match pattern that ends with es"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jBM1EJQrKbO"
      },
      "source": [
        "[w for w in tokens1 if re.search('es$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHrrdJ6u06n1"
      },
      "source": [
        "Extract pattern with numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N680i0HFrKbO"
      },
      "source": [
        "[w for w in tokens1 if re.search('[0-9]', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slmgmSX_rKbP"
      },
      "source": [
        "# Lower case\n",
        "Converting the tokens to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbKBFederKbP"
      },
      "source": [
        "tokens = [token.lower() for token in tokens]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLdgjtuqrKbP"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4p8w-tarKbQ"
      },
      "source": [
        "A stop word is a commonly used word (such as \"a\", \"an“, \"it”, “in”, “the”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
        "\n",
        "We would not want these words taking up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words. spacy in python has a list of stopwords stored in many different languages. You can find them in the spacy directory.\n",
        "\n",
        "Note: You can even modify the list by adding words of your choice in the english .txt. file in the stopwords directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8zfLLJwrKbQ"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "print(STOP_WORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACst0E8I4fKQ"
      },
      "source": [
        "Stopword removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff_YHHfR4XJB"
      },
      "source": [
        "tokens = [token for token in tokens if token not in STOP_WORDS]\n",
        "print(len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "372gFP-CrKbQ"
      },
      "source": [
        "# Lemmatizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66X7EG2DrKbR"
      },
      "source": [
        "Lemmas are root form of a word. It is helpful to reduce the bag of words by using the same root word for all similar kind of words.\n",
        "\n",
        "* Lemmatizers use a corpus. The result is always a dictionary word.\n",
        "* Lemmatizers need extra info about the part of speech they are processing.\n",
        "\n",
        "Note: spaCy adds a special case for English pronouns: all English pronouns are lemmatized to the special token -PRON-. Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, -PRON-, which is used as the lemma for all personal pronouns.\n",
        "\n",
        "https://spacy.io/usage#pron-lemma\n",
        "\n",
        "https://spacy.io/api/annotation#lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQyyU68rKbR"
      },
      "source": [
        "tokens_new = nlp(\"going gone go goes went\")\n",
        "\n",
        "# Print the text and the predicted tags\n",
        "print([(w.text, w.lemma_) for w in tokens_new])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUkNncfrrKbS"
      },
      "source": [
        "plurals = nlp(\"Indian caresses flies dies education denied computer computing xyzing done slept\")\n",
        "\n",
        "# Print the text and the predicted tags\n",
        "print([(w.text, w.lemma_) for w in plurals])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5G6qlSWrKbT"
      },
      "source": [
        "# Print the text and the predicted tags\n",
        "print([(w.lemma_) for w in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1FLArd2rKbT"
      },
      "source": [
        "#### Let's combine all the above commands into a single function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hKdZIiCrKbU"
      },
      "source": [
        "def process_text(doc):\n",
        "\n",
        "    tokens = [token.text for token in doc]\n",
        "    tokens = [token for token in tokens if token not in STOP_WORDS]\n",
        "\n",
        "    doc_new = nlp(\" \".join(tokens))\n",
        "\n",
        "    tokens_lemma = [w.lemma_ for w in doc_new]\n",
        "\n",
        "    return tokens_lemma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8qwdT6nrKbU"
      },
      "source": [
        "print(process_text(nlp(string)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUqTUzH1rKbU"
      },
      "source": [
        "``` python\n",
        "!pip install wordcloud\n",
        "\n",
        "  (or)\n",
        "\n",
        "!conda install -c conda-forge wordcloud=1.6.0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6SFFnQ5rKbU"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "SCb_7QECEBCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YjSumQDIrKbU"
      },
      "source": [
        "wordcloud = WordCloud(max_font_size=80, max_words=25, background_color=\"lavender\").generate(string)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "606n1ejTrKbV"
      },
      "source": [
        "# Reference links\n",
        "\n",
        "https://spacy.io/usage\n",
        "\n",
        "https://spacy.io/usage/rule-based-matching\n",
        "\n",
        "https://spacy.io/universe/project/textacy\n",
        "\n",
        "https://chartbeat-labs.github.io/textacy/build/html/api_reference/information_extraction.html#textacy.extract.ngrams\n",
        "\n",
        "https://docs.python.org/3/library/re.html\n",
        "\n",
        "https://spacy.io/usage/rule-based-matching\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0lwmA8QkO_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}